{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ee4a20",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5577d5b",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to leverage transfer learning with a pretrained large language model (LLM) like RoBERTa Large. We begin with an instance of the model that has already been fine-tuned on a corpus such as Multi-Genre Natural Language Inference (MNLI), and show how to fine-tune it further on a related corpus, e.g. Stanford Natural Language Inference (SNLI). This process prepares the model for eventual fine-tuning on a task-specific dataset, such as SciEntsBank for Automated Short-Answer Grading. Here, we are transferring knowledge from MNLI and SNLI to the target task.\n",
    "\n",
    "Fine-tuning a model on a second corpus differs slightly from fine-tuning it on a single corpus or a task-specific dataset. When fine-tuning on another corpus, it is important to consider several factors to ensure the process is aligned and effective. This notebook highlights these key aspects to ensure a smoother transfer of knowledge and improved performance on the target task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d743880",
   "metadata": {},
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For hardware acceleration\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# For Hugging Face\n",
    "%pip install transformers datasets accelerate\n",
    "\n",
    "# For metrics\n",
    "%pip install scikit-learn numpy\n",
    "\n",
    "# For Notebook Widgets\n",
    "%pip install ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc4662",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c76ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'stanfordnlp/snli'\n",
    "model_name = 'FacebookAI/roberta-large-mnli'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47322583",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a097b9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0110e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md: 100% [====================]  16.0k/16.0k [00:00<00:00, 2.39MB/s]\n",
      "test-00000-of-00001.parquet: 100% [====================]  412k/412k [00:00<00:00, 14.7MB/s]\n",
      "validation-00000-of-00001.parquet: 100% [====================]  413k/413k [00:00<00:00, 60.4MB/s]\n",
      "train-00000-of-00001.parquet: 100% [====================]  19.6M/19.6M [00:00<00:00, 210MB/s]\n",
      "Generating test split: 100% [====================]  10000/10000 [00:00<00:00, 801571.69 examples/s]\n",
      "Generating validation split: 100% [====================]  10000/10000 [00:00<00:00, 757573.20 examples/s]\n",
      "Generating train split: 100% [====================]  550152/550152 [00:00<00:00, 2238690.21 examples/s]"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571408b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 550152\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d4771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Map: {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n"
     ]
    }
   ],
   "source": [
    "print('Label Map:', {index: label for index, label in enumerate(dataset['train'].features['label'].names)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab91fe",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "\n",
    "It is important to omit `num_labels` when loading the model to ensure that the classification head retains its fine-tuned weights from the MNLI corpus rather than being reinitialized with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b6531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json: 100% [====================] 688/688 [00:00<00:00, 134kB/s]\n",
      "model.safetensors: 100% [====================] 1.43G/1.43G [00:06<00:00, 229MB/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e1b5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Map: {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n"
     ]
    }
   ],
   "source": [
    "print('Label Map:', model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86a4ed",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281059f",
   "metadata": {},
   "source": [
    "### Filter Out Unlabeled Examples\n",
    "\n",
    "The dataset card states, \"Dataset instances which don't have any gold label are marked with -1 label. Make sure you filter them before starting the training...\" Although the label map shows only three distinct labels, we can identify the fourth label by retrieving the list of unique labels directly from the examples. It is crucial to inspect the labels, as such discrepancies may not always be documented in the dataset card, and the labels must align with the previous corpus and the current label map of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc80dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in examples: {0, 1, 2, -1}\n"
     ]
    }
   ],
   "source": [
    "print('Labels in examples:', set(dataset['train']['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654020cf",
   "metadata": {},
   "source": [
    "We need to filter out the unlabeled examples from each split before we can feed the data to the model. To filter the examples with our target labels, we need to specify the labels by their IDs (rather than their names) as they appear in the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66a50ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter:  100%  [====================]  10000/10000 [00:00<00:00, 290742.12 examples/s]\n",
      "Filter:  100%  [====================]  10000/10000 [00:00<00:00, 436229.60 examples/s]\n",
      "Filter:  100%  [====================]  550152/550152 [00:01<00:00, 518269.03 examples/s]"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda example: example['label'] in [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "224d3076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9824\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9842\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 549367\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33f07707",
   "metadata": {},
   "source": [
    "### Align Labels\n",
    "\n",
    "The label map of the dataset is not aligned with the label map of the model. For example, while the label `0` represents *contradiction* in the model, it corresponds to *entailment* in the dataset. Aligning the labels between the model and the dataset is essential to ensure they interpret label indices consistently. Without alignment, the fine-tuning process will train the model on incorrect label relationships and compromise the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "397bb8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Before Alignment: [1, 2, 0, 1, 0, 2, 2, 0, 1, 1]\n",
      "Label Map Before Alignment: {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n"
     ]
    }
   ],
   "source": [
    "print('Labels Before Alignment:', dataset['train']['label'][:10])\n",
    "print('Label Map Before Alignment:', {index: label for index, label in enumerate(dataset['train'].features['label'].names)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e688b105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning the labels: 100% [====================] 9824/9824 [00:00<00:00, 128011.07 examples/s]\n",
      "Aligning the labels: 100% [====================] 9842/9842 [00:00<00:00, 129070.44 examples/s]\n",
      "Aligning the labels: 100% [====================] 549367/549367 [00:03<00:00, 134994.16 examples/s]"
     ]
    }
   ],
   "source": [
    "dataset = dataset.align_labels_with_mapping({'contradiction': 0, 'neutral': 1, 'entailment': 2}, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d538b8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels After Alignment: [1, 0, 2, 1, 2, 0, 0, 2, 1, 1]\n",
      "Label Map After Alignment: {0: 'contradiction', 1: 'neutral', 2: 'entailment'}\n"
     ]
    }
   ],
   "source": [
    "print('Labels After Alignment:', dataset['train']['label'][:10])\n",
    "print('Label Map After Alignment:', {index: label for index, label in enumerate(dataset['train'].features['label'].names)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f161b",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56dc89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd16549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100% [====================] 25.0/25.0 [00:00<00:00, 8.34kB/s]\n",
      "vocab.json: 100% [====================] 899k/899k [00:00<00:00, 8.79MB/s]\n",
      "merges.txt: 100% [====================] 456k/456k [00:00<00:00, 16.0MB/s]\n",
      "tokenizer.json: 100% [====================] 1.36M/1.36M [00:00<00:00, 4.86MB/s]"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenization_function(example):\n",
    "    return tokenizer(text = example['premise'], text_pair = example['hypothesis'], truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf834f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100% [====================] 9824/9824 [00:00<00:00, 20828.77 examples/s]\n",
      "Map: 100% [====================] 9842/9842 [00:00<00:00, 75362.91 examples/s]\n",
      "Map: 100% [====================] 549367/549367 [00:07<00:00, 73286.77 examples/s]"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(tokenization_function, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412d0ad",
   "metadata": {},
   "source": [
    "# Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc644e",
   "metadata": {},
   "source": [
    "### Data Collator\n",
    "\n",
    "The data collator batches the examples and pads the input sequences to the same length, ensuring compatibility with the model during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "430f47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716455c4",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "To evaluate the performance of the model during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "853e32cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51e4549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(labels):\n",
    "    # Unpack predicted and true labels\n",
    "    y_pred, y_true = labels\n",
    "    \n",
    "    # Convert logits (predicted probabilities) to class labels\n",
    "    # by selecting the index with the highest probability.\n",
    "    y_pred = np.argmax(y_pred, axis = 1)\n",
    "    \n",
    "    # Calculate metrics by comparing the predicted labels against the true labels\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average = 'macro')\n",
    "    \n",
    "    # Return the calculated metrics\n",
    "    return {\n",
    "        'Acc': acc,\n",
    "        'F1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ab7ef",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Monitors the evaluation metric during training and halts the process early if performance does not improve for a specified number of evaluation steps, helping to prevent overfitting and conserve computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa5ed433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience = 3,\n",
    "    early_stopping_threshold = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658b014",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "Manages the entire fine-tuning process, including training, evaluation, optimization, logging, and checkpoint creation to simplify the model training and ensuring efficient execution.\n",
    "\n",
    "We are using the same hyperparameters that were applied during the fine-tuning on the MNLI corpus, except for the number of epochs and the batch size. The number of epochs is set to 10, which is unlikely to be reached due to early stopping, as training will halt when no significant improvement is observed in the evaluation metric. The batch size is determined by the available memory of the computational hardware. For this setup, we have set the batch size to 64, as we will utilize eight NVIDIA RTX A4000 GPUs to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "028c40a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 10,\n",
    "    per_device_train_batch_size = 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    \n",
    "    adam_epsilon = 1e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.98,\n",
    "    \n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.1,\n",
    "    lr_scheduler_type = 'linear',\n",
    "    warmup_ratio = 0.06,\n",
    "    \n",
    "    eval_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    metric_for_best_model = 'F1',\n",
    "    greater_is_better = True,\n",
    "    \n",
    "    output_dir = 'checkpoints',\n",
    "    overwrite_output_dir = True,\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 4,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset = dataset['validation'],\n",
    "    processing_class = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c308b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3107df95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10730' max='21460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10730/21460 3:34:29 < 2:22:51, 1.25 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.219194</td>\n",
       "      <td>0.926844</td>\n",
       "      <td>0.926414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.215655</td>\n",
       "      <td>0.930197</td>\n",
       "      <td>0.929720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.223093</td>\n",
       "      <td>0.930502</td>\n",
       "      <td>0.930309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.247766</td>\n",
       "      <td>0.930400</td>\n",
       "      <td>0.930355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>0.257386</td>\n",
       "      <td>0.929384</td>\n",
       "      <td>0.929248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "[==========          ] [10730/21460 3:34:29 < 2:22:51, 1.25 it/s, Epoch 5/10]\n",
       "\n",
       "Epoch   Training Loss   Validation Loss         Acc          F1\n",
       "    1        0.290200          0.219194    0.926844    0.926414\n",
       "    2        0.229000          0.215655    0.930197    0.929720\n",
       "    3        0.182200          0.223093    0.930502    0.930309\n",
       "    4        0.145700          0.247766    0.930400    0.930355\n",
       "    5        0.118300          0.257386    0.929384    0.929248"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10730, training_loss=0.02365808975774333, metrics={'train_runtime': 12869.618, 'train_samples_per_second': 3204.103, 'train_steps_per_second': 12.516, 'total_flos': 3.152317832477303e+17, 'train_loss': 0.02365808975774333, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce0088ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 4\n"
     ]
    }
   ],
   "source": [
    "print('Best Epoch:', int(int(trainer.state.best_model_checkpoint.split('/')[-1].split('-')[-1]) / (trainer.state.max_steps / trainer.state.num_train_epochs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a70f6",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6d27054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "CONTRADICTION       0.95      0.95      0.95      3237\n",
      "      NEUTRAL       0.89      0.90      0.89      3219\n",
      "   ENTAILMENT       0.93      0.92      0.93      3368\n",
      "\n",
      "     accuracy                           0.92      9824\n",
      "    macro avg       0.92      0.92      0.92      9824\n",
      " weighted avg       0.92      0.92      0.92      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "results = trainer.predict(dataset['test'])\n",
    "\n",
    "print(classification_report(\n",
    "    y_true = results.label_ids,\n",
    "    y_pred = np.argmax(results.predictions, axis = 1),\n",
    "    target_names = dataset['train'].features['label'].names\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39175dd3",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56101a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the model\n",
    "output_dir = 'roberta-large-mnli-snli'\n",
    "\n",
    "# Creates a draft model card.\n",
    "# The model card is a README.md file that will be saved\n",
    "# to the directory specified by training_args.output_dir.\n",
    "trainer.create_model_card(\n",
    "    model_name = 'roberta-large-mnli-snli',\n",
    "    language = 'EN',\n",
    "    license = 'mit',\n",
    "    tags = ['RoBERTa Large', 'MNLI', 'SNLI'],\n",
    "    finetuned_from = 'FacebookAI/roberta-large-mnli',\n",
    "    tasks = ['RTE'],\n",
    "    dataset_tags = ['MNLI', 'SNLI'],\n",
    "    dataset = ['stanfordnlp/snli']\n",
    ")\n",
    "\n",
    "# Save the model including the tokenizer\n",
    "trainer.save_model(output_dir = output_dir)\n",
    "\n",
    "# Save the trainer state to resume training in the future\n",
    "trainer.state.save_to_json(f'{output_dir}/trainer_state.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
