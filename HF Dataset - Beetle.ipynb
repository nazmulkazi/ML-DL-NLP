{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beetle dataset was released in XML format. This notebook outlines the steps taken to convert it into a Hugging Face dataset.\n",
    "\n",
    "While converting the dataset, we have made a couple of design decisions: (1) We included only the 5-way labels and provided Python code to convert them into 3-way or 2-way labels. (2) We reformatted the identifiers. (3) The dataset contains multiple reference answers of varying standards for each question. Including a column with lists as values would complicate data processing and model training, as most functions and language models expect scalar values. Typically, only one reference answer per question would be used in most NLP tasks. Therefore, we selected the reference answer rated as the best by the authors for each question. During the filtering process, we observed that some questions had multiple reference answers rated as the best. In such cases, we chose the first instance of the best-rated reference answer. However, we created a separate set containing all the reference answers along with their rated standards for researchers who are interested in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsing XML files\n",
    "%pip install beautifulsoup4 lxml\n",
    "\n",
    "# For creating Hugging Face dataset\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is provided in XML format, distributed across 103 files, with each file corresponding to a single question in the set. Each file contains a question, reference answers, and student answers along with their associated labels. Our goal is to parse the information from these XML files and organize them into a unified structure.\n",
    "\n",
    "To accomplish this, we will use the Beautiful Soup library to parse the XML files. In addition to extracting the primary information, we need to collect and store supplementary metadata (e.g., module names and IDs) to trace each piece of information back to its source if needed in the future. Each question is identified by a unique ID generated from keywords in the question, such as `BULB_C_VOLTAGE_EXPLAIN_WHY1`. Answer IDs (e.g., `FaultFinding-BULB_C_VOLTAGE_EXPLAIN_WHY1.sbj3-l1.qa193`) follow a specific structure, incorporating the module name, question ID, subject ID, subject level, and answer number in sequence. We will abbreviate module names, index question IDs, and reformat both question and answer IDs to create shorter and uniform identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviate the module names\n",
    "# md stands for metadata\n",
    "md_modules = {\n",
    "    'FaultFinding': 'FF',\n",
    "    'SwitchesBulbsParallel': 'PC', # Parallel Circuit\n",
    "    'SwitchesBulbsSeries': 'SC' # Series Circuit\n",
    "}\n",
    "\n",
    "# Index question ids from each module in alphabetical order\n",
    "# We will reference them by their index (1-based)\n",
    "md_question_ids = {\n",
    "    'FF': [\n",
    "        'BULB_C_VOLTAGE_EXPLAIN_WHY1', 'BULB_C_VOLTAGE_EXPLAIN_WHY2', 'BULB_C_VOLTAGE_EXPLAIN_WHY4', 'BULB_C_VOLTAGE_EXPLAIN_WHY6', 'BULB_ONLY_EXPLAIN_WHY2', 'BULB_ONLY_EXPLAIN_WHY4', 'BULB_ONLY_EXPLAIN_WHY6', 'BURNED_BULB_LOCATE_EXPLAIN_Q', 'DESCRIBE_GAP_LOCATE_PROCEDURE_Q', 'OTHER_TERMINAL_STATE_EXPLAIN_Q', 'TERMINAL_STATE_EXPLAIN_Q', 'VOLTAGE_AND_GAP_DISCUSS_Q', 'VOLTAGE_DEFINE_Q', 'VOLTAGE_DIFF_DISCUSS_1_Q', 'VOLTAGE_DIFF_DISCUSS_2_Q', 'VOLTAGE_ELECTRICAL_STATE_DISCUSS_Q', 'VOLTAGE_GAP_EXPLAIN_WHY1', 'VOLTAGE_GAP_EXPLAIN_WHY2', 'VOLTAGE_GAP_EXPLAIN_WHY3', 'VOLTAGE_GAP_EXPLAIN_WHY4', 'VOLTAGE_GAP_EXPLAIN_WHY5', 'VOLTAGE_GAP_EXPLAIN_WHY6', 'VOLTAGE_INCOMPLETE_CIRCUIT_2_Q'\n",
    "        ],\n",
    "    'PC': [\n",
    "        'BURNED_BULB_PARALLEL_EXPLAIN_Q1', 'BURNED_BULB_PARALLEL_EXPLAIN_Q2', 'BURNED_BULB_PARALLEL_EXPLAIN_Q3', 'BURNED_BULB_PARALLEL_WHY_Q', 'GIVE_CIRCUIT_TYPE_HYBRID_EXPLAIN_Q2', 'GIVE_CIRCUIT_TYPE_HYBRID_EXPLAIN_Q3', 'GIVE_CIRCUIT_TYPE_PARALLEL_EXPLAIN_Q2', 'HYBRID_BURNED_OUT_EXPLAIN_Q1', 'HYBRID_BURNED_OUT_EXPLAIN_Q2', 'HYBRID_BURNED_OUT_EXPLAIN_Q3', 'HYBRID_BURNED_OUT_WHY_Q1', 'HYBRID_BURNED_OUT_WHY_Q2', 'HYBRID_BURNED_OUT_WHY_Q3', 'OPT1_EXPLAIN_Q2', 'OPT2_EXPLAIN_Q', 'PARALLEL_SWITCH_EXPLAIN_Q1', 'PARALLEL_SWITCH_EXPLAIN_Q2', 'PARALLEL_SWITCH_EXPLAIN_Q3', 'SWITCH_TABLE_EXPLAIN_Q1', 'SWITCH_TABLE_EXPLAIN_Q2', 'SWITCH_TABLE_EXPLAIN_Q3'\n",
    "        ],\n",
    "    'SC': [\n",
    "        'BURNED_BULB_SERIES_Q2', 'CLOSED_PATH_EXPLAIN', 'CONDITIONS_FOR_BULB_TO_LIGHT', 'DAMAGED_BUILD_EXPLAIN_Q', 'DAMAGED_BULB_EXPLAIN_2_Q', 'DAMAGED_BULB_SWITCH_Q', 'GIVE_CIRCUIT_TYPE_SERIES_EXPLAIN_Q', 'SHORT_CIRCUIT_EXPLAIN_Q_2', 'SHORT_CIRCUIT_EXPLAIN_Q_4', 'SHORT_CIRCUIT_EXPLAIN_Q_5', 'SHORT_CIRCUIT_X_Q', 'SWITCH_OPEN_EXPLAIN_Q'\n",
    "        ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_id(answer_id: str) -> str:\n",
    "    '''\n",
    "    Reformats answer ids.\n",
    "    \n",
    "    Args:\n",
    "        answer_id (str): Original answer id.\n",
    "    \n",
    "    Returns:\n",
    "        str: Reformatted answer id.\n",
    "    '''\n",
    "    \n",
    "    # Split the answer id into segments\n",
    "    segments = list(re.fullmatch(r'([a-z]+)-([^\\.]+)\\.([^-]+)-([^\\.]+)\\.q(a\\d+)', answer_id, flags=re.IGNORECASE).groups())\n",
    "    # Replace the module name\n",
    "    segments[0] = md_modules[segments[0]]\n",
    "    # Replace the question id with its index (1-based)\n",
    "    segments[1] = 'q' + str(md_question_ids[segments[0]].index(segments[1]) + 1)\n",
    "    # Change \"qa\" to \"sa\" to align with the term \"student answer\"\n",
    "    segments[-1] = 's' + segments[-1]\n",
    "    # Combine all modified segments into a single string separated by periods and convert it to uppercase\n",
    "    return '.'.join(segments).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the data, with one key for each set and a dedicated key to list all the reference answers\n",
    "data = {\n",
    "    'train': [],\n",
    "    'test_ua': [],\n",
    "    'test_uq': [],\n",
    "    'all_reference_answers': []\n",
    "}\n",
    "\n",
    "# File location of each set\n",
    "data_map = {\n",
    "    'train': 'Raw/train/5-way/',\n",
    "    'test_ua': 'Raw/test/5-way/test-unseen-answers/',\n",
    "    'test_uq': 'Raw/test/5-way/test-unseen-questions/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the files and load the data\n",
    "for set_name in data_map:\n",
    "    # Traverse through the files in each set\n",
    "    for file in os.scandir(data_map[set_name]):\n",
    "        # Parse XML files\n",
    "        if file.is_file() and file.name.endswith('.xml'):\n",
    "            with open(file.path, 'r') as file:\n",
    "                xml = BeautifulSoup(file, 'xml')\n",
    "            root = xml.find('question')\n",
    "            \n",
    "            # Extract and convert the module name\n",
    "            module = md_modules[root.get('module')]\n",
    "            # Extract and convert the question id\n",
    "            question_id = md_question_ids[module].index(root.get('id')) + 1\n",
    "            # Extract the question\n",
    "            question = xml.find('questionText').text\n",
    "            \n",
    "            # Extract and store all the reference answers except for the \"ua\" set since the questions of the \"ua\" set is a subset of train\n",
    "            if set_name == 'test_ua':\n",
    "                # Extract the first best reference answer to be included in samples\n",
    "                reference_answer = xml.find('referenceAnswer', attrs={'category': 'BEST'}).text\n",
    "            else:\n",
    "                # Variable to store the first best reference answer, to be included in samples later.\n",
    "                # NOTE: There can be more than one best reference answer for the same question.\n",
    "                reference_answer = None\n",
    "                # Start a new counter to generate ids for the reference answers in the order they are listed\n",
    "                ra_id = 1\n",
    "                # Iterate through the reference answer elements\n",
    "                for el in xml.find_all('referenceAnswer'):\n",
    "                    # Extract the standatd/quality of the answer\n",
    "                    standard = el.get('category').lower()\n",
    "                    # Skip if it is not an answer but rather a keyword\n",
    "                    if standard == 'keyword': continue\n",
    "                    # Check and store the first best reference answer\n",
    "                    if not reference_answer and standard == 'best':\n",
    "                        reference_answer = el.text\n",
    "                    # Store the reference answer as a sample in its dedicated set\n",
    "                    data['all_reference_answers'].append({\n",
    "                        'id': f'{module}.Q{question_id}.RA{ra_id}',\n",
    "                        'question': question,\n",
    "                        'reference_answer': el.text,\n",
    "                        'standard': standard\n",
    "                    })\n",
    "                    # Increment the counter\n",
    "                    ra_id += 1\n",
    "            \n",
    "            # Extract the student answers and store each as a sample\n",
    "            for el in xml.find_all('studentAnswer'):\n",
    "                data[set_name].append({\n",
    "                    'id': format_id(el.get('id')),\n",
    "                    'question': question,\n",
    "                    'reference_answer': reference_answer,\n",
    "                    'student_answer': el.text,\n",
    "                    'label': el.get('accuracy')\n",
    "                })\n",
    "    \n",
    "    # Sort the samples by their ids\n",
    "    data[set_name].sort(key=lambda e: e['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# For preview\n",
    "with open('Beetle.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# For scripts\n",
    "with open('Beetle.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset for Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we started building the dataset, we created a new dataset in our Hugging Face account and cloned the repository locally to a directory named `HuggingFace`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Beetle.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "from datasets import Features\n",
    "from datasets import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the internal structure of the dataset.\n",
    "# NOTE: The class labels are not in alphabetical order since it is\n",
    "# important to preserve their conceptual relationship and direction.\n",
    "features_default = Features({\n",
    "    'id': Value('string'),\n",
    "    'question': Value('string'),\n",
    "    'reference_answer': Value('string'),\n",
    "    'student_answer': Value('string'),\n",
    "    'label': ClassLabel(names=['correct', 'contradictory', 'partially_correct_incomplete', 'irrelevant', 'non_domain'])\n",
    "})\n",
    "\n",
    "# We created a dedicated set for all the reference answers.\n",
    "# This set has a different internal structure and needs to be defined independently.\n",
    "# ara stands for \"all reference answers\"\n",
    "features_ara = Features({\n",
    "    'id': Value('string'),\n",
    "    'question': Value('string'),\n",
    "    'reference_answer': Value('string'),\n",
    "    'standard': ClassLabel(names=['minimal', 'good', 'best'])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data Into Datasets\n",
    "\n",
    "Please note that the term \"dataset\" (not the variable) in this section refers to a single set/split and not the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 571.92ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 500.87ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 333.78ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 500.93ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the datasets\n",
    "dataset = {}\n",
    "\n",
    "# Ensure the directory exist to export the datasets.\n",
    "# All data files should be stored in the \"data\" subdirectory, following standard practice.\n",
    "os.makedirs('HuggingFace/data/', exist_ok=True)\n",
    "\n",
    "# Iterate through each set\n",
    "for set_name in data:\n",
    "    # Transform the set into a dataset\n",
    "    dataset[set_name] = Dataset.from_list(data[set_name], features=features_ara if set_name == 'all_reference_answers' else features_default)\n",
    "    # Export the dataset in Parquest format\n",
    "    dataset[set_name].to_parquet(f'HuggingFace/data/{set_name.replace(\"_\", \"-\")}-00001.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "    num_rows: 3941\n",
      "}), 'test_ua': Dataset({\n",
      "    features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "    num_rows: 439\n",
      "}), 'test_uq': Dataset({\n",
      "    features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "    num_rows: 819\n",
      "}), 'all_reference_answers': Dataset({\n",
      "    features: ['id', 'question', 'reference_answer', 'standard'],\n",
      "    num_rows: 242\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "# Overview of the datasets\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Metadata For Readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "dataset_info:\n",
      "  features:\n",
      "  - name: id\n",
      "    dtype: string\n",
      "  - name: question\n",
      "    dtype: string\n",
      "  - name: reference_answer\n",
      "    dtype: string\n",
      "  - name: student_answer\n",
      "    dtype: string\n",
      "  - name: label\n",
      "    dtype:\n",
      "      class_label:\n",
      "        names:\n",
      "          '0': correct\n",
      "          '1': contradictory\n",
      "          '2': partially_correct_incomplete\n",
      "          '3': irrelevant\n",
      "          '4': non_domain\n",
      "  splits:\n",
      "  - name: train\n",
      "    num_examples: 3941\n",
      "    num_bytes: 120274\n",
      "  - name: test_ua\n",
      "    num_examples: 439\n",
      "    num_bytes: 21208\n",
      "  - name: test_uq\n",
      "    num_examples: 819\n",
      "    num_bytes: 27339\n",
      "  dataset_size: 168821\n",
      "configs:\n",
      "- config_name: default\n",
      "  data_files:\n",
      "  - split: train\n",
      "    path: data/train-*\n",
      "  - split: test_ua\n",
      "    path: data/test-ua-*\n",
      "  - split: test_uq\n",
      "    path: data/test-uq-*\n",
      "- config_name: all_reference_answers\n",
      "  data_files:\n",
      "  - split: all_reference_answers\n",
      "    path: data/all-reference-answers-*\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Begin metadata section\n",
    "print('---')\n",
    "\n",
    "# Print dataset information section\n",
    "print('dataset_info:')\n",
    "\n",
    "# Print dataset features\n",
    "print('  features:')\n",
    "for name, value in features_default.items():\n",
    "    print('  - name:', name)\n",
    "    if name == 'label':\n",
    "        print(' '*3, 'dtype:')\n",
    "        print(' '*5, 'class_label:')\n",
    "        print(' '*7, 'names:')\n",
    "        for i, name in enumerate(value.names):\n",
    "            print(' '*9, f\"'{i}': {name}\")\n",
    "    else:\n",
    "        print(' '*3, 'dtype:', value.dtype)\n",
    "\n",
    "# Print dataset splits metadata and calculate dataset size\n",
    "print('  splits:')\n",
    "dataset_size = 0\n",
    "for set_name in dataset:\n",
    "    # Skip \"all_reference_answers\" since it is not part of the default subset\n",
    "    if set_name == 'all_reference_answers':\n",
    "        continue\n",
    "    \n",
    "    print('  - name:', set_name)\n",
    "    print(' '*3, 'num_examples:', len(dataset[set_name]))\n",
    "    num_bytes = os.stat(f'HuggingFace/data/{set_name.replace(\"_\", \"-\")}-00001.parquet').st_size\n",
    "    print(' '*3, 'num_bytes:', num_bytes)\n",
    "    dataset_size += num_bytes\n",
    "\n",
    "# Print dataset size\n",
    "print('  dataset_size:', dataset_size)\n",
    "\n",
    "# Print data file configurations\n",
    "print('configs:')\n",
    "# Define config for the default subset\n",
    "print('- config_name: default')\n",
    "print('  data_files:')\n",
    "for set_name in dataset:\n",
    "    # Skip \"all_reference_answers\" since we will put it in a separate subset\n",
    "    if set_name == 'all_reference_answers':\n",
    "        continue\n",
    "    \n",
    "    print('  - split:', set_name)\n",
    "    print(' '*3, 'path:', f'data/{set_name.replace(\"_\", \"-\")}-*')\n",
    "\n",
    "# Define a separate subset/config for the all_reference_answers set\n",
    "print('''\n",
    "- config_name: all_reference_answers\n",
    "  data_files:\n",
    "  - split: all_reference_answers\n",
    "    path: data/all-reference-answers-*\n",
    "'''.strip())\n",
    "\n",
    "# End metadata section\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Dataset to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Copy the generated metadata from this notebook into the README.md file.\n",
    "2. Use the Metadata UI on the Hugging Face website to populate the remaining metadata (e.g., dataset name, license, task categories, etc.), then copy the generated text into the README.md file.\n",
    "3. Populate the README.md file with information about the dataset, including instructions, label distribution, citation, references, and more.\n",
    "4. Commit and push the changes using Git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset URL: [https://huggingface.co/datasets/nkazi/Beetle](https://huggingface.co/datasets/nkazi/Beetle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
