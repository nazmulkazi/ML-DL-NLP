{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SciEntsBank dataset was released in XML format. This notebook outlines the steps taken to convert it into a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsing XML files\n",
    "%pip install beautifulsoup4 lxml\n",
    "\n",
    "# For creating Hugging Face dataset\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is provided in XML format, distributed across 331 files, with each file corresponding to a single question in the set. Each file contains a question, a reference answer, and student answers along with their associated labels. Our goal is to parse the information from these XML files and organize them into a unified structure.\n",
    "\n",
    "To accomplish this, we will use the Beautiful Soup library to parse the XML files. In addition to the primary information, we will also extract and store answer identifiers, allowing us to trace each answer back to its source if needed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the data, with a key for each set\n",
    "data = {\n",
    "    'train': [],\n",
    "    'test_ua': [],\n",
    "    'test_uq': [],\n",
    "    'test_ud': []\n",
    "}\n",
    "\n",
    "# File location of each set\n",
    "data_map = {\n",
    "    'train': 'Raw/train/5-way/',\n",
    "    'test_ua': 'Raw/test/5-way/test-unseen-answers/',\n",
    "    'test_uq': 'Raw/test/5-way/test-unseen-questions/',\n",
    "    'test_ud': 'Raw/test/5-way/test-unseen-domains/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the files and load the data\n",
    "for set_name in data_map:\n",
    "    # Traverse through the files in each set\n",
    "    for file in os.scandir(data_map[set_name]):\n",
    "        # Parse XML files\n",
    "        if file.is_file() and file.name.endswith('.xml'):\n",
    "            with open(file.path, 'r') as file:\n",
    "                xml = BeautifulSoup(file, 'xml')\n",
    "            root = xml.find('question')\n",
    "            \n",
    "            # Extract question\n",
    "            question = xml.find('questionText').text\n",
    "            \n",
    "            # Extract reference answer\n",
    "            reference_answer = xml.find_all('referenceAnswer')\n",
    "            # Check whether multiple reference answers exist\n",
    "            if len(reference_answer) > 1:\n",
    "                print('[ WARNING ]  Found more than one reference answer in', file.path)\n",
    "            reference_answer = reference_answer[0].text\n",
    "            \n",
    "            # Extract student answers and store each as a sample\n",
    "            for el in xml.find_all('studentAnswer'):\n",
    "                data[set_name].append({\n",
    "                    'id': el.get('id'),\n",
    "                    'question': question,\n",
    "                    'reference_answer': reference_answer,\n",
    "                    'student_answer': el.text,\n",
    "                    'label': el.get('accuracy')\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# For preview\n",
    "with open('SciEntsBank.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# For scripts\n",
    "with open('SciEntsBank.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset for Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we started building the dataset, we created a new dataset in our Hugging Face account and cloned the repository locally to a directory named `HuggingFace`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('SciEntsBank.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "from datasets import Features\n",
    "from datasets import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the internal structure of the dataset.\n",
    "# NOTE: The class labels are not in alphabetical order since it is\n",
    "# important to preserve their conceptual relationship and direction.\n",
    "features = Features({\n",
    "    'id': Value('string'),\n",
    "    'question': Value('string'),\n",
    "    'reference_answer': Value('string'),\n",
    "    'student_answer': Value('string'),\n",
    "    'label': ClassLabel(names=['correct', 'contradictory', 'partially_correct_incomplete', 'irrelevant', 'non_domain'])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data Into Datasets\n",
    "\n",
    "Please note that the term \"dataset\" (not the variable) in this section refers to a single set/split and not the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 1250.09ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 749.26ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 999.83ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 2499.88ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the datasets\n",
    "dataset = {}\n",
    "\n",
    "# Ensure the directory exist to export the datasets.\n",
    "# All data files should be stored in the \"data\" subdirectory, following standard practice.\n",
    "os.makedirs('HuggingFace/data/', exist_ok=True)\n",
    "\n",
    "# Iterate through each set\n",
    "for set_name in data:\n",
    "    # Transform the set into a dataset\n",
    "    dataset[set_name] = Dataset.from_list(data[set_name], features=features)\n",
    "    # Export the dataset in Parquest format\n",
    "    dataset[set_name].to_parquet(f'HuggingFace/data/{set_name.replace(\"_\", \"-\")}-00001.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "    num_rows: 4969\n",
      "}), 'test_ua': Dataset({\n",
      "    features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "    num_rows: 540\n",
      "}), 'test_uq': Dataset({\n",
      "    features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "    num_rows: 733\n",
      "}), 'test_ud': Dataset({\n",
      "    features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "    num_rows: 4562\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "# Overview of the datasets\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset Card For Readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "dataset_info:\n",
      "  features:\n",
      "  - dtype: string\n",
      "    name: id\n",
      "  - dtype: string\n",
      "    name: question\n",
      "  - dtype: string\n",
      "    name: reference_answer\n",
      "  - dtype: string\n",
      "    name: student_answer\n",
      "  - dtype:\n",
      "      class_label:\n",
      "        names:\n",
      "          '0': correct\n",
      "          '1': contradictory\n",
      "          '2': partially_correct_incomplete\n",
      "          '3': irrelevant\n",
      "          '4': non_domain\n",
      "    name: label\n",
      "  splits:\n",
      "  - name: train\n",
      "    num_examples: 4969\n",
      "    num_bytes: 232655\n",
      "  - name: test_ua\n",
      "    num_examples: 540\n",
      "    num_bytes: 52730\n",
      "  - name: test_uq\n",
      "    num_examples: 733\n",
      "    num_bytes: 35716\n",
      "  - name: test_ud\n",
      "    num_examples: 4562\n",
      "    num_bytes: 177307\n",
      "  dataset_size: 498408\n",
      "configs:\n",
      "- config_name: default\n",
      "  data_files:\n",
      "  - split: train\n",
      "    path: data/train-*\n",
      "  - split: test_ua\n",
      "    path: data/test-ua-*\n",
      "  - split: test_uq\n",
      "    path: data/test-uq-*\n",
      "  - split: test_ud\n",
      "    path: data/test-ud-*\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Begin metadata section\n",
    "print('---')\n",
    "\n",
    "# Print dataset information section\n",
    "print('dataset_info:')\n",
    "\n",
    "# Print dataset features\n",
    "print('  features:')\n",
    "print(re.sub(r'^', '  ', yaml.safe_dump(features._to_yaml_list()), flags=re.MULTILINE))\n",
    "\n",
    "# Print dataset splits metadata and calculate dataset size\n",
    "print('  splits:')\n",
    "dataset_size = 0\n",
    "for set_name in dataset:\n",
    "    print('  - name:', set_name)\n",
    "    print(' '*3, 'num_examples:', len(dataset[set_name]))\n",
    "    num_bytes = os.stat(f'HuggingFace/data/{set_name.replace(\"_\", \"-\")}-00001.parquet').st_size\n",
    "    print(' '*3, 'num_bytes:', num_bytes)\n",
    "    dataset_size += num_bytes\n",
    "\n",
    "# Print dataset size\n",
    "print('  dataset_size:', dataset_size)\n",
    "\n",
    "# Print data file configurations\n",
    "print('configs:')\n",
    "# Define config for the default subset\n",
    "print('- config_name: default')\n",
    "print('  data_files:')\n",
    "for set_name in dataset:\n",
    "    print('  - split:', set_name)\n",
    "    print(' '*3, 'path:', f'data/{set_name.replace(\"_\", \"-\")}-*')\n",
    "\n",
    "# End metadata section\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Dataset to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Copy the generated metadata from this notebook into the README.md file.\n",
    "2. Use the Metadata UI on the Hugging Face website to populate the remaining metadata (e.g., dataset name, license, task categories, etc.), then copy the generated text into the README.md file.\n",
    "3. Populate the README.md file with information about the dataset, including instructions, label distribution, citation, references, and more.\n",
    "4. Commit and push the changes using Git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset URL: [https://huggingface.co/datasets/nkazi/SciEntsBank](https://huggingface.co/datasets/nkazi/SciEntsBank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
