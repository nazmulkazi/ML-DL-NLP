{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ee4a20",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5577d5b",
   "metadata": {},
   "source": [
    "This notebook presents a minimal example of fine-tuning a pretrained large language model (LLM), such as RoBERTa Large, on a task-specific dataset like SciEntsBank for Automated Short-Answer Grading (multi-class classification) using the Hugging Face library. This notebook is outlined for beginners to provide an easy-to-follow high-level overview of the fine-tuning process.\n",
    "\n",
    "**Disclaimer:** Some of the choices made in this demonstration deviate from standard practices, made solely to keep the notebook simple and at an introductory level. These choices are highlighted throughout the notebook to notify learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d743880",
   "metadata": {},
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For hardware acceleration\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# For Hugging Face\n",
    "%pip install transformers datasets accelerate\n",
    "\n",
    "# For metrics\n",
    "%pip install scikit-learn numpy\n",
    "\n",
    "# For Notebook Widgets\n",
    "%pip install ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc4662",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global Variables\n",
    "\n",
    "In this section, we define variables to store the dataset and model names. We will use these variables to reference the names throughout the notebook. This way, we will need to update the name only in one place if we want to use a different dataset or model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c76ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'nkazi/SciEntsBank'\n",
    "model_name = 'FacebookAI/roberta-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47322583",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "In this section, we load the dataset and preprocess it for model training. The dataset will be tokenized, which is a crucial step to convert raw text into a format that the model understands. Tokenization splits the text into smaller units (tokens) and maps them to numerical representations, allowing the model to process and learn from the data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf7129",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c0110e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md: 100% [====================] 7.93k/7.93k [00:00<00:00, 1.24MB/s]\n",
      "train-00001.parquet: 100% [====================] 233k/233k [00:00<00:00, 7.30MB/s]\n",
      "test-ua-00001.parquet: 100% [====================] 52.7k/52.7k [00:00<00:00, 848kB/s]\n",
      "test-uq-00001.parquet: 100% [====================] 35.7k/35.7k [00:00<00:00, 8.36MB/s]\n",
      "test-ud-00001.parquet: 100% [====================] 177k/177k [00:00<00:00, 33.5MB/s]\n",
      "Generating train split: 100% [====================] 4969/4969 [00:00<00:00, 158096.13 examples/s]\n",
      "Generating test_ua split: 100% [====================] 540/540 [00:00<00:00, 97073.73 examples/s]\n",
      "Generating test_uq split: 100% [====================] 733/733 [00:00<00:00, 129728.04 examples/s]\n",
      "Generating test_ud split: 100% [====================] 4562/4562 [00:00<00:00, 509761.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571408b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "        num_rows: 4969\n",
      "    })\n",
      "    test_ua: Dataset({\n",
      "        features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "        num_rows: 540\n",
      "    })\n",
      "    test_uq: Dataset({\n",
      "        features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "        num_rows: 733\n",
      "    })\n",
      "    test_ud: Dataset({\n",
      "        features: ['id', 'question', 'reference_answer', 'student_answer', 'label'],\n",
      "        num_rows: 4562\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7e14d",
   "metadata": {},
   "source": [
    "Based on the printout, the dataset contains four splits: one training split and three test splits. All splits share the same features (columns), and the number of examples (rows) is listed for each split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e93a90",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "Each model has its own tokenizer, which should be used to process the examples. In this case, we aim to train the model to grade student answers by comparing them to reference answers. A reference answer serves as context and should be provided as `text`, while the student answer, which we want to classify, should be placed in `text_pair`. The tokenizer processes both and combines them into a single input. We instruct the tokenizer to truncate long inputs, if necessary, to stay within the model's input size limit. We define this tokenization process in a function and apply it to the entire dataset using the `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccd48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca993b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100% [====================] 25.0/25.0 [00:00<00:00, 8.34kB/s]\n",
      "vocab.json: 100% [====================] 899k/899k [00:00<00:00, 8.79MB/s]\n",
      "merges.txt: 100% [====================] 456k/456k [00:00<00:00, 16.0MB/s]\n",
      "tokenizer.json: 100% [====================] 1.36M/1.36M [00:00<00:00, 4.86MB/s]"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenization_function(example):\n",
    "    return tokenizer(text = example['reference_answer'], text_pair = example['student_answer'], truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870fd567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100% [====================] 4969/4969 [00:00<00:00, 32667.89 examples/s]\n",
      "Map: 100% [====================] 540/540 [00:00<00:00, 23202.15 examples/s]\n",
      "Map: 100% [====================] 733/733 [00:00<00:00, 25146.00 examples/s]\n",
      "Map: 100% [====================] 4562/4562 [00:00<00:00, 40214.66 examples/s]"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(tokenization_function, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab91fe",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "\n",
    "We define two label mappings, `id2label` and `label2id`, to convert between label names and their corresponding identifiers. Next, we load the model, specifying the number of labels and providing the mappings, which reinitializes the classification head to adapt the model for our dataset. The model uses these mappings to correctly interpret and convert labels during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7582f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {index: label for index, label in enumerate(dataset['train'].features['label'].names)}\n",
    "label2id = {label: key for key, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b6531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json: 100% [====================] 482/482 [00:00<00:00, 99.0kB/s]\n",
      "model.safetensors: 100% [====================] 1.42G/1.42G [00:07<00:00, 207MB/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path = model_name,\n",
    "    num_labels = len(id2label),\n",
    "    id2label = id2label,\n",
    "    label2id = label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412d0ad",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "In this section, we define the necessary resources for fine-tuning the model and then proceed to fine-tune it on the prepared dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc644e",
   "metadata": {},
   "source": [
    "### Data Collator\n",
    "\n",
    "The data collator batches the examples and pads the input sequences to the same length as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430f47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716455c4",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "We define a function, as required, to take the predicted and true labels as input and return the desired metrics to evaluate the model's performance during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "853e32cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e4549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(labels):\n",
    "    # Unpack predicted and true labels\n",
    "    y_pred, y_true = labels\n",
    "    \n",
    "    # Convert logits (predicted probabilities) to class labels\n",
    "    # by selecting the index with the highest probability.\n",
    "    y_pred = np.argmax(y_pred, axis = 1)\n",
    "    \n",
    "    # Calculate metrics by comparing the predicted labels against the true labels\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average = 'macro')\n",
    "    \n",
    "    # Return the calculated metrics\n",
    "    return {\n",
    "        'Acc': acc,\n",
    "        'F1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658b014",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "The trainer manages the entire fine-tuning process, simplifying model training and ensuring efficient execution. In this example, we set only the most important hyperparameters using `TrainingArguments`. If you choose to fine-tune just one hyperparameter, it should be the learning rate. We set the weight decay to help prevent overfitting. An epoch is one complete pass through the entire training dataset during the training process. The number of epochs should be chosen based on the dataset size, task complexity and model performance. The batch size is determined by the available memory on your computational hardware (e.g., GPU). It is standard practice to evaluate and log metrics after each epoch to track the model's progress during training. We configure the trainer to create a checkpoint after each epoch in the `checkpoints` directory, retain up to four checkpoints with the top F1 scores, and return the model from the checkpoint that achieved the highest F1 score.\n",
    "\n",
    "When initializing the `Trainer`, we choose `test_ua` as the validation (i.e., evaluation) set. Ideally, a dataset should have a dedicated validation set separate from the test set, but many datasets, including this one, do not. This dataset contains three different types of test sets, with `test_ua` being the typical test set included in datasets. We choose to use this test set for both validation and testing purposes in this example. If the dataset includes a validation set, it should be used for validation, and the test set should never be exposed to the model until training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "028c40a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    \n",
    "    num_train_epochs = 12,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    \n",
    "    eval_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    \n",
    "    output_dir = 'checkpoints',\n",
    "    overwrite_output_dir = True,\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 4,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'F1',\n",
    "    greater_is_better = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset = dataset['test_ua'],\n",
    "    processing_class = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c308b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train\n",
    "\n",
    "We train the model and evaluate its performance across epochs, then identify the epoch where the model achieved the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3107df95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1872' max='1872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1872/1872 35:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.184600</td>\n",
       "      <td>1.021545</td>\n",
       "      <td>0.572222</td>\n",
       "      <td>0.368131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.822200</td>\n",
       "      <td>0.899404</td>\n",
       "      <td>0.642593</td>\n",
       "      <td>0.491488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.591100</td>\n",
       "      <td>0.890760</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.535872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>1.004231</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.520210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>1.010344</td>\n",
       "      <td>0.690741</td>\n",
       "      <td>0.530876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>1.156096</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.541642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>1.242854</td>\n",
       "      <td>0.701852</td>\n",
       "      <td>0.542842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>1.367253</td>\n",
       "      <td>0.720370</td>\n",
       "      <td>0.629825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>1.416527</td>\n",
       "      <td>0.718519</td>\n",
       "      <td>0.630617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.542099</td>\n",
       "      <td>0.709259</td>\n",
       "      <td>0.541595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>1.648238</td>\n",
       "      <td>0.705556</td>\n",
       "      <td>0.545231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>1.668828</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>0.557181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "[====================] [1872/1872 35:58, Epoch 12/12]\n",
       "\n",
       "Epoch   Training Loss   Validation Loss         Acc          F1\n",
       "    1        1.184600          1.021545    0.572222    0.368131\n",
       "    2        0.822200          0.899404    0.642593    0.491488\n",
       "    3        0.591100          0.890760    0.703704    0.535872\n",
       "    4        0.443900          1.004231    0.677778    0.520210\n",
       "    5        0.340600          1.010344    0.690741    0.530876\n",
       "    6        0.256900          1.156096    0.688889    0.541642\n",
       "    7        0.194700          1.242854    0.701852    0.542842\n",
       "    8        0.148800          1.367253    0.720370    0.629825\n",
       "    9        0.106300          1.416527    0.718519    0.630617\n",
       "   10        0.088400          1.542099    0.709259    0.541595\n",
       "   11        0.070600          1.648238    0.705556    0.545231\n",
       "   12        0.067300          1.668828    0.725926    0.557181"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1872, training_loss=0.22679513956491765, metrics={'train_runtime': 2160.3945, 'train_samples_per_second': 46.001, 'train_steps_per_second': 0.722, 'total_flos': 1.5687663623032722e+16, 'train_loss': 0.22679513956491765, 'epoch': 12.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce0088ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "print('Best Epoch:', int(int(trainer.state.best_model_checkpoint.split('/')[-1].split('-')[-1]) / (trainer.state.max_steps / trainer.state.num_train_epochs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a70f6",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "In this section, we evaluate the trained model on the `test_ua` set. We use the trainer to make predictions and then employ the SciKit Learn library to generate a report featuring four commonly used metrics: accuracy, precision, recall, and F1 score. Repeat the process to evaluate the model on the other test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6d27054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                     correct       0.81      0.81      0.81       233\n",
      "               contradictory       0.62      0.62      0.62        58\n",
      "partially_correct_incomplete       0.54      0.66      0.60       113\n",
      "                  irrelevant       0.81      0.66      0.73       133\n",
      "                  non_domain       0.50      0.33      0.40         3\n",
      "\n",
      "                    accuracy                           0.72       540\n",
      "                   macro avg       0.66      0.62      0.63       540\n",
      "                weighted avg       0.73      0.72      0.72       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "results = trainer.predict(dataset['test'])\n",
    "\n",
    "print(classification_report(\n",
    "    y_true = results.label_ids,\n",
    "    y_pred = np.argmax(results.predictions, axis = 1),\n",
    "    target_names = dataset['train'].features['label'].names\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ca0a1",
   "metadata": {},
   "source": [
    "# Save Model\n",
    "\n",
    "Fine-tuning an LLM requires time and considerable computational resources. Saving the trained model allows us to reuse it later without retraining. However, the model file can be quite large, depending on the size of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56101a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the model\n",
    "output_dir = 'RoBERTa_Large_SciEntsBank'\n",
    "\n",
    "# Save the trained model along with the tokenizer\n",
    "trainer.save_model(output_dir = output_dir)\n",
    "\n",
    "# Save the trainer state to resume training in the future\n",
    "trainer.state.save_to_json(f'{output_dir}/trainer_state.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2f57c",
   "metadata": {},
   "source": [
    "# Export Predictions\n",
    "\n",
    "As a best practice, we highly recommend exporting the predictions, especially if the trained model wasn't saved. This allows us to compute different metrics and analyze performance in various ways without retraining or re-running the model. We should avoid dumping the `results` object directly into a pickle file, as it often fails to load properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f0197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = {key: getattr(results, key) for key in ['predictions', 'label_ids', 'metrics']}\n",
    "\n",
    "with open('predictions_test_ua.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
